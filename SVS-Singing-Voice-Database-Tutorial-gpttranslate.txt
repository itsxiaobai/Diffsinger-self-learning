.

作者：PixPrucer（如果有任何不清楚的地方，請聯絡我！）
（或寄信給我）

⚠️ 注意事項：這份教學不是教科書，而是過程紀錄。
請依照每一章節的內容參考與模仿，使用你自己的素材來實作達成相同的結果。

感謝 Spoopy*Ace 協助完成這份文件！

📢 如果你需要幫助，可以加入 DiffSinger 的 Discord 伺服器
📚 如果你想要更詳細的解說，這裡有完整的 DiffSinger 教學

⸻

Dummy Breakdown

進入兔子洞！！

⸻

專有名詞辭典
	•	SVS： “Singing Voice Synthesis（歌聲合成）” 的縮寫
	•	UTAU： 一款免費的拼接式歌聲合成軟體，在聲音合成社群中被廣泛使用
	•	LAB（檔案）： 一種包含語音標籤與時間點資訊的檔案，副檔名為 .lab，是開源 SVS 引擎常用的標準格式

⸻

Step 0 — 資料庫規劃階段

🚫 不要直接開始錄音，先好好規劃！

我會以自己的聲音資料集為例進行說明。

⸻

🗣️ 你想包含哪種語言？
我希望以 波蘭語 為主要語言，並加入 日語 作為次要語言。

⸻

🔤 你想使用哪一種音素系統？標準的？還是自訂的？
我使用了 波蘭語 UTAU 音素系統，因為它穩定且直覺。
隨著時間，我也加入了一些額外音素，使模型功能更完整。

你可以先在 TXT 檔中列出音素清單，方便標註時不會混亂。

⸻

這些音素當然是針對波蘭語的。
英語、日語或西班牙語的音素會有所不同。

請注意：音素集可以依你需要自由設計，
但 不是所有音素都能與 OpenUTAU 內建音素轉換器（phonemiser）相容。
（[此處有音素系統連結，Prucer]）

如果你想在同一模型中整合多種語言，
務必確認不同語言的音素不會彼此衝突。
若兩種語音的發音相同，可以合併；
但若發音不同，則應分開處理。

⸻

🎵 你希望聲音的語氣是什麼？自然？還是有表演感？
我只錄了我自然的歌聲，沒有特別誇張的表現。 :]

⸻

🔊 你想錄製不同的聲音模式（vocal modes）嗎？
你可以錄製不同的語氣與音色，例如：
柔和（soft）、強烈（strong）、沙啞（raspy）、有力（powerful）、低吼（growly）、耳語（whispery）等！
要包含哪些聲音模式完全取決於你。

值得注意的是：
每一種聲音模式都需要獨立的資料夾，
每個資料夾中都應包含「wav」與「lab」兩個子資料夾。
🎶 你希望形成什麼特定習慣嗎？
（例如：刻意在某些音域只使用頭聲/假聲，使其更具語境性）

我在語料庫中同時包含了 頭聲、胸聲和混聲，因此模型會根據使用的音域自動切換聲音技巧。
不過，如果你使用 vocalmode 資料夾分割，你可以錄製單獨的聲音技巧，之後可以單獨使用，而不是讓 AI 根據語境選擇聲音技巧。
這樣你可以強制使用強而有力的胸聲，或者非常低沉、耳語般的假聲。

⸻

🎵 選擇你想唱的歌曲！
建立播放清單，加入你已經熟悉的歌曲！
你會驚訝於這樣累積的資料量。
之後務必過濾歌曲，去除以下情況的曲目：
	•	太難唱的
	•	歌詞太少
	•	大部分是「ooh」或「yeah」等無意義聲音

對於困難片段，要事先規劃如何演唱：
	•	低唱或高唱
	•	快唱或慢唱
	•	或乾脆不唱

最重要的是！不要重複任何片段。
AI 需要多樣化、變化豐富的資料才能學得好。
同一段副歌唱兩三次對模型幫助不大，因為它只是重複的模式。
你需要不斷給 AI 新鮮的「驚喜」才能學習得更好。

⸻

🏷️ 你打算自己標註還是委託幫忙？
標註是一件艱難且耗時的事情，一分鐘的音訊平均手動標註需要 30 分鐘。
如果你覺得自己沒有足夠時間，可以委託我！
更多資訊請見下方說明。

⸻

🎹 你打算做自動調音（autopitch）模型嗎？
自動調音模型並非必要，你的聲音模型本身即可正常運作。
這只是額外功能，可以讓輸入的樂譜以你的聲音風格更自然地呈現。

這些考量主要針對額外功能，不會破壞你的語料庫，但提前規劃仍然有幫助。 ~Ace

⸻

Step 1 — 錄音

“我不會唱歌啦”
你只是對自己不夠有信心 ❤️

每個歌聲資料庫的第一步都是 資料收集（錄音）。
錄音方法就是唱不同的歌曲。
這裡不需要什麼固定曲單（reclist）。
你選擇錄的歌曲本身就可以當作你的「reclist」。

我建議使用 cobalt 下載原曲或伴奏，作為錄音參考軌道。
這樣更容易跟隨旋律和節奏。

錄音長度可以隨意，從幾秒到幾分鐘都可以。

⸻

⚠️ 小提醒
	•	不用太擔心跑調。
DiffSinger 可以使用你原本的音準作為輸入，完全忽略原唱跑調。
	•	請不要使用自動調音/ Melodyne 處理錄音
	•	這會產生音高轉換引擎噪音，反而降低 AI 訓練品質。
	•	若打算訓練 autopitch 模型，DiffSinger 會自動補償原唱跑調，因此提前調音也沒必要。
	•	混響會影響後續品質，且難以去除
	•	儘量在安靜的環境錄音。
	•	可使用：
	•	衣物堆滿的衣櫥
	•	有枕頭或毯子的房間
	•	車內
	•	請降低麥克風增益
	•	避免錄音出現討厭的破音（clipping）。

我會使用 Audacity，因為它可以完整準備語料庫。
當然，你也可以選擇其他錄音軟體。

Step 1a — 編輯

值得注意的是，AI 會盡力複製你的輸入資料，因此 清理錄音對於獲得高品質結果非常重要。

你可以先匯出未編輯的錄音，再使用其他你喜歡的音訊清理軟體進行處理，例如 Izotope RX。
不一定非得用 Audacity，但 Audacity 方便的是可以在同一個軟體完成整個語料庫的處理流程。

⸻

我的清理流程如下：
	1.	降噪（Denoise）
	•	先從錄音中選擇一段靜音片段，取得乾淨的噪音輪廓（noise profile）。
	•	接著對整段錄音執行降噪。
	•	設定 Reduction（降噪強度） 為 10，Sensitivity（靈敏度） 為 2。
	•	連續執行這個效果 3 次（如果噪音仍在，可以再多次執行）。

	2.	均衡（EQ）
在 Audacity 中稱為 「Filter Curve（濾波曲線）」。
選擇內建的 「Low Rolloff for Speech（語音低頻削減）」 預設，並對整段錄音套用即可。
3.	修剪（Trimming）
完成前面的步驟後，我還會 修剪錄音中冗長的靜音段落和不理想的片段，
將資料盡量濃縮，只保留真正的歌聲部分。
🎉 完成啦！你的錄音現在已經乾淨、準備好使用了

接下來，你可以把錄音 儲存到 WAV 資料夾，並隨意命名：
	•	可以用數字
	•	可以用小寫字母（不要空格）
	•	或兩者組合

❌ 注意：不要使用特殊符號或日文字。

確保匯出的音訊格式為 單聲道（mono）、44.1 kHz、16 位元。
小資料量也足夠，模型就能表現不錯

只要 兩首完整歌曲（約 5 分鐘），模型就已經能唱得還不錯！
你可以依照可用或想錄的時間，決定錄音長度：
	•	15 分鐘
	•	30 分鐘
	•	60 分鐘
基本上就是 你能錄多少就錄多少。

常見做法是以 15 分鐘為單位 計算語料庫長度。

注意：在 兩小時錄音量之後，音質不會有明顯提升 ——
這時聲音會稍微穩定一些，但不會變得更好，除非新增的資料包含 先前語料庫中沒有的範例。

⸻

Step 2 — 標註（Labelling）

啊…痛苦的開始 😅
（不過說實話，有點放鬆的痛苦 — Ace）

這個部分對 英文和日文 以及其他支援語言可以自動化處理。
labelmakr 工具可以大幅加快標註流程。

當你收集到一些歌聲樣本後，就該開始 標註它們（這非常耗費心力）。

標註資料是資料庫建立中另一個關鍵步驟。
它告訴 AI 每個 音素（phoneme）在歌聲中的位置。
精準很重要，因為標記不準可能會在後續造成問題。
（個人看法：一致性比單純追求「絕對精準」更重要，但精準度仍然要注意 😆 — Ace）

標註就像是為你的 聲音資料庫程式化：
資料庫的功能取決於你如何配置標註。
（記住！請依照你希望語音庫行為的方式來標註資料 :3 — Ace）

社群主要使用的標註軟體是 vLabeler：
	•	易於操作
	•	原生支援 LAB 格式

你可以在官方教學中詳細了解如何使用，我在這裡只做 NNSVS 標註功能的粗略介紹，這是本教學將使用的方法。

⸻

Step 2b — 在 vLabeler 中

（影片來源：UTA）

設定過程相當直觀，容易上手。

在將所有資料夾路徑對應到各自位置後，點擊 ⚙️ 設定按鈕，
並設定語料庫中 對應正確檔案類型的資料夾：

現在，你已經準備好開始 標註 了！
當然，要在 vLabeler 完成音訊波形渲染之後才能開始。

將專案中的 第一個標籤（label）重新命名為 [pau]，
這在之後會很有用，作為 預設音素 使用。

使用 剪刀工具（scissors tool），將每個音素標註對應的 語音符號（phonetic sound）。
	•	右鍵點擊：播放該段音訊
	•	左鍵點擊：放置標籤
	•	輸入對應的語音符號，然後移動滑鼠到下一個位置

下面示範了如何使用 羅馬拼音（romaji）音素集 標註 日語。

（注意：這裡的視窗縮小是為了方便截圖。你可以將視窗設為 全螢幕，以獲得最佳標註精準度）

⚠️ 注意事項：
	•	「ん」 不論發音如何，都標註為 N。
AI 會根據後續子音自動判斷何時以及如何變化（基本上是根據語境）。
	•	「ky」子音標籤 不應該包含 y 的音，只保留 k。
y 的音應該放入元音部分（vowel）。

And English, using the Arpabet phoneme set, English labelling guide

注意：每個 樂句之間的停頓 都要標註為 [pau]。
它就像音樂中的句點一樣，用來表示停頓。
同時，也要把 吸氣（breath intake） 標註在這裡！

下面是 [cl] 在「きっと (kitto)」中的示範用法。

偶爾出現的隨意或具有風格性的發音可以收錄進資料庫中，
但必須標註為它們「原本應該對應的音素」。

在這裡，歌手將日文的 「r」 唸成了較長且柔的 「l」 音，而非較硬的「r」音。
另外請注意，母音 「a」 的結尾帶有一個呼氣聲，
你應該將這個呼氣聲包含在 「a」 母音的標籤內，
而不是將其標為停頓（pause）

當結尾的呼氣聲 非常長 時，
你可以將它標註為 「h」。

當你完成所有標註後，
可以在選單中使用 「Export Overwriting All（全部覆寫匯出）」 選項來匯出所有標籤。
匯出的標籤檔應該會出現在 「lab」資料夾 中。

⚠️ 注意：
每次在資料庫中新增錄音時，都必須建立一個 新的 vLabeler 專案。
vLabeler 不會自動更新錄音清單。

⸻

Step 2a — 在 Audacity 中

首先，在你的歌聲軌道下方 插入一個標籤軌（label track）。
然後 複製你的聲音軌道（vocal track），
以便能同時查看其 頻譜圖（spectrogram）。

你也可以調整 頻譜圖（spectrogram）設定，
讓畫面更清晰、方便觀察細節。

以下是我所使用的設定：

接著，在音軌的開頭放置每種語言都會使用的 第一個通用音素「pau」。
它表示 樂句中的停頓，同時也包含 吸氣聲（breath intake）。

在音軌的結尾放置一個 「END」標籤。
這個標籤會告訴轉換腳本（conversion script）
應該在何處設定最後一個停頓的長度。

接著，逐步回放你的歌聲，
並根據你先前準備的 音素清單（phoneme list），
在每個音素的開頭位置放置標記（marker）進行標註。

在標籤欄中輸入對應的 語音符號（phonetic character） 即可插入標籤，
並可透過 拖曳中間的小圓點 來移動標籤位置。

⚠️ 記得在每個 樂句結尾 或 吸氣處（breath intake） 插入一個 「pau」標籤！

這裡示範我使用 小型羅馬拼音音素集（small Romaji phonetic set）
來標註我剛錄製的日語歌聲。

⸻

🎥 完整影片示範流程

這種方法的一大優點是：
如果有不滿意的片段，你可以直接刪除那一段，
並同時移動整組標籤，使它們相對於其他標註音訊的時間保持一致。

⸻

💾 另外，別忘了儲存進度！
你可以將專案儲存在 語料庫（corpus）資料夾 中，
或是建立一個 專用的專案資料夾 來保存你的工作。

現在，你需要 將標籤匯出成檔案，
這樣它們才不會被鎖在 Audacity 裡。
將它們匯出到 「lab」資料夾，並確保 檔名與對應的 WAV 檔案相符。

⸻

接下來，為了確保標籤符合 所需格式，
你需要使用 轉換腳本（conversion script） 進行轉換。
這個腳本可以從這裡下載。

之前放置的 END 標籤 在這裡就派上用場了，
轉換完成後可以將它刪除。

⸻

Step 3 — 資料庫打包（Database packaging）

由於我們使用 Google Colab 來訓練模型，
你需要將資料庫打包到 你的 Google Drive。
	1.	建立一個新的資料夾，例如命名為 「Database」，
這個資料夾用來存放 已準備好訓練的檔案。
	2.	在裡面複製一份資料庫資料夾，並依照以下方式整理：
	•	刪除專案檔案
	•	將所有 WAV 和 LAB 檔案放到資料夾外層
	•	刪除原本的 wav 和 lab 資料夾
	•	（可選）刪除 TXT 標籤
	3.	對所有包含不同附加資料或聲音表現的子資料夾，
同樣依照上述步驟處理。

⸻

完成整理後，選取所有資料夾並壓縮（ZIP）。

Step 4 — 訓練（Training）

這部分以 Colab 訓練 為例說明，
本地端訓練（Local Training）可另行查閱指南。

Colab 訓練資源庫（Repo）

技術上來說，這份教學到建立 SVS 資料庫就可以結束了，
因為名稱就說明了重點是 製作資料庫，而非完整的歌聲模型。
不過為了方便，我也會說明訓練過程。

⸻

如何操作 Colab 筆記本？
	1.	首先，執行 setup cell。
	2.	允許 Google 存取你的 Google Drive，因為資料庫就存放在那裡。

接著，你需要 將資料庫解壓到 Colab runtime。

需要修改的設定如下：
	•	data_zip_path：/content/drive/MyDrive/_database.zip（指向你的資料庫 ZIP 路徑）
	•	estimate_midi：先開啟以訓練 Variance，之後訓練 Acoustics 時關閉
	•	segment_length：訓練片段長度（秒），建議降低至 10 秒
	•	max_silence_phoneme_amount：每段中允許的靜音音素數量，我通常設為 1（較短片段）

⸻

Step 4a — Variance

此模型用於決定 Variance 參數，例如音高與節奏。
必須先訓練此模型。

需要修改的設定如下：
	•	config_type：variance
	•	save_dir：/content/drive/MyDrive/DiffSinger/MyModel_Variance
	•	f0_ext：parselmouth / harvest
	•	data_aug：false（GIF 範例中忘了關掉，不影響）

訓練時，將 TensorBoard 的資料夾設定為模型資料夾：
	•	logs：/content/drive/MyDrive/DiffSinger/MyModel_Variance

開始訓練，耐心等待。
建議 50k steps，可多或少，依你對模型結果滿意度而定。

⚠️ 注意：TensorBoard 顯示的結果通常比原始音訊稍差，但模型最終會接近輸入資料，有些誤差範圍是正常的。

⸻

Step 4b — Acoustic

修改設定如下：
	•	config_type：acoustic
	•	save_dir：/content/drive/MyDrive/DiffSinger/MyModel_Acoustic
	•	f0_ext：parselmouth / harvest
	•	data_aug：true（可設為 false，但小資料庫開啟會更穩定）

訓練流程與 Variance 相同，
記得將 TensorBoard 路徑改成 Acoustic 模型

資料夾：
	•	logs：/content/drive/MyDrive/DiffSinger/MyModel_Acoustic

開始訓練，再次耐心等待。

⚠️ 注意：TensorBoard 中呈現的音訊片段都經過 vocoder 處理，包括 ground truth（標記為 gt），
所以如果聽起來比原始錄音品質低，不必擔心，這是為了方便比較模型表現。

⸻

Step 4c — ONNX

模型訓練完成後，需要匯出為 ONNX 格式，方便部署。
此格式也 OpenUTAU 所需。

在 Exporting 區域，從檔案總管中複製並貼上 Acoustic 與 Variance 的 model_().ckpt 路徑
到對應變數中，並指定匯出的資料夾。

完成後，就可以開始製作 OpenUTAU 聲音庫（voicebank）。
此步驟需要從不同位置複製檔案，因此建議先將資料夾全部打開並準備好。

處理完成後，你的 voicebank ZIP 將會出現在 Google Drive 等待下載！

⸻

Step 5 — OpenUTAU 整合
	1.	下載完成 voicebank 後，打開 OpenUTAU
	2.	點選 Tools > Install Singer…
	3.	從檔案總管選取 voicebank，完成安裝

安裝後，你可能就可以直接使用，
但有時會遇到 phonemiser 錯誤、渲染錯誤 或其他無法理解的錯誤。
這時需要檢查 OU logs，自行修正或請伺服器支援。
錯誤原因可能很多，對應的修復方式也各有不同。

⸻

超級額外複雜步驟（SECS）

如果你想：
	•	確認標註是否正確
	•	或在本地端轉換資料集，為 Autopitch 做更完善的準備

可依以下步驟操作：

資料集驗證（Dataset verification）
	1.	完成 Step 3，資料庫就準備好進行其他操作
	2.	資料夾內檔案應如下所示
	3.	下載 UTA 的資料集工具
	4.	解壓縮 ZIP 檔
	5.	將 check_labels.py 複製到你的資料庫資料夾中並執行
	6.	執行後會列出 標註錯誤清單

⚠️ 預設清單是針對日語，但你可以自由修改，檢查任何音素集。

Autopitch ++

安裝（Installation）

首先，你需要下載三個資源庫：
	1.	MakeDiffSinger
	2.	Dataset Tools
	3.	Uta 的 Database Converter

	•	從 MakeDiffSinger 資源庫下載 ZIP 格式的程式碼
	•	Dataset Tools：下載最新版本
	•	Uta 的 Database Converter：下載最新版本

將它們解壓到一個已知資料夾，最好放在一起，方便操作。

⸻

資料集轉換（Dataset conversion）

當你的標註完成後，需要將資料集 轉換成 DiffSinger 格式，
接下來我們將在此格式上進行後續操作。
	1.	打開 nnsvs_db_converter 資料夾。
	2.	找到 lang.sample.json 檔案，複製一份並根據你的標籤清單修改。
	•	預設是日語配置，如果你的資料庫標註的是日語音素，則無需更改。
	•	這裡也提供一份 英文語言定義檔 作為範例。
	3.	在 nnsvs_db_converter 資料夾中打開 CMD，輸入以下指令：

db_converter.py -s 1 -L [你的 json 檔名] -m [你的資料庫路徑]

修正（Correction）

現在，我們將對資料庫進行一些操作。
	1.	同時打開你的 資料庫資料夾 與 MakeDiffSinger 資料夾。
	2.	在 MakeDiffSinger 視窗中，找到 variance-temp-solution 資料夾，我們將使用裡面的腳本。
	3.	在 variance-temp-solution 資料夾中打開 CMD，第一個使用的腳本是 correct_cents.py，
它會讓 MIDI 更精準，減少錯誤校正的需求。

執行指令如下：

correct_cents.py csv [CSV 檔案路徑] [對應 WAV 檔案路徑]

完成後不要關閉 CMD，我們接著執行下一個腳本。

⸻

轉換（Conversion）

在 CMD 中輸入以下指令：

convert_ds.py csv2ds [CSV 檔案路徑] [對應 WAV 檔案路徑]

這個腳本會將轉錄檔（transcriptions）轉換成 DS MIDI 檔，
接下來可以在外部程式編輯它們。

⸻

SlurCutter
	1.	在 dataset-tools 資料夾中找到 SlurCutter，打開後編輯器會跳出。
	2.	點選 File > Open Folder，選擇已轉換資料庫中 WAV 所在資料夾。
	3.	開始檢查所有 MIDI 片段。

	•	在瀏覽 MIDI 時，系統會自動覆蓋檔案
	•	如果一個音符持續時間有多個段落，雙擊它即可分割為更小片段，提高 MIDI 精準度

⚠️ 注意：MIDI 音符不必完美置中於音高波動的中心，
之後可以在 OpenUTAU 中強制音高置中。

⸻

再轉換（Re-Conversion）

檢查完成 MIDI 後，重新執行轉換腳本，這次反向操作：

convert_ds.py ds2csv [對應 WAV 檔案路徑] [CSV 檔案路徑]

•	這次會將 transcriptions.csv 輸出為 transcriptions_new.csv
	•	確保檔案不互相衝突，舊檔案可刪除或重新命名作為備份

現在，你的資料庫已經為 Autopitch 訓練做好額外準備！

⸻

資源（Resource）

錄音（Recording）
	•	Audacity

標註（Labelling）
	•	vLabeler
	•	Wavesurfer
	•	Labelmakr
	•	SOFA
	•	Automatic labelling notebook
	•	Automatic labelling GUI

文件（Documentation）
	•	Arpabet phoneme set
	•	English labelling guide

訓練（Training）
	•	Training notebook
	•	Local training guide

OpenUTAU 部署（OU deployment）
	•	OpenUTAU
	•	Manual deployment documentation

DiffSinger 資源
	•	DiffSinger GitHub

⸻

委託（Commissions）

價格依據委託者提供的素材量而定（USD，個人）。

項目	價格（每分鐘）
音訊清理與準備	$1–4 / 分鐘
手動標註	$6–12 / 分鐘
樂譜打分（Autopitch）	$1–3 / 分鐘
總資料準備費用	$8–20 / 分鐘
DiffSinger 模型訓練	$5 / 10k steps

	•	價格依錄音資料的複雜性與困難度而異
	•	委託所需錄音必須是 未處理的原始單聲道 WAV，格式 48kHz/24bit
	•	支援語言：英文、日文、西班牙語、中文、波蘭語
	•	若錄音語言不同，需額外收費（因需學習標註方式）

委託示例
	•	PIX DS 3
	•	Peiton DS
	•	No.7
	•	D.A.I.chi
	•	LIEE : Virtual Idol
	•	BAQULOiD
	•	Wanda DS
	•	Violette DS



